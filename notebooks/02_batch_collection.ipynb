{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02 - Batch Data Collection\n",
    "\n",
    "Efficiently collect data for multiple Stadium project candidates.\n",
    "\n",
    "**Goals:**\n",
    "1. Load Stadium candidates from `data/stadium_candidates.md`\n",
    "2. Quick verification of maintainer counts\n",
    "3. Batch collection with rate limit management\n",
    "4. Progress tracking and error handling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os\nimport sys\nimport json\nimport time\nfrom pathlib import Path\nfrom datetime import datetime\n\nimport pandas as pd\nfrom dotenv import load_dotenv\n\n# Add src to path\nsys.path.insert(0, '../src')\nfrom collection.github_collector import GitHubCollector\n\n# Load environment from .env file\nenv_path = Path(\"../.env\")\nif env_path.exists():\n    load_dotenv(env_path)\n    print(f\"‚úÖ Loaded .env from {env_path.resolve()}\")\nelse:\n    load_dotenv()\n    print(\"‚ö†Ô∏è  No .env file found, trying default locations\")\n\nGITHUB_TOKEN = os.getenv('GITHUB_TOKEN')\n\nif not GITHUB_TOKEN:\n    raise ValueError(\n        \"GITHUB_TOKEN not found!\\n\"\n        \"1. Copy .env.example to .env: cp ../.env.example ../.env\\n\"\n        \"2. Edit .env and add your GitHub token\\n\"\n        \"3. Get a token at: https://github.com/settings/tokens\"\n    )\n\nif GITHUB_TOKEN == \"your_github_token_here\":\n    raise ValueError(\n        \"GITHUB_TOKEN is still the placeholder!\\n\"\n        \"Edit ../.env and replace with your actual token\"\n    )\n\n# Initialize collector\ncollector = GitHubCollector(token=GITHUB_TOKEN)\n\nprint(\"‚úÖ Setup complete!\")\nrate = collector.get_rate_limit()\nprint(f\"   Rate limit: {rate['core']['remaining']}/{rate['core']['limit']}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Define Stadium Candidates\n",
    "\n",
    "Based on research criteria:\n",
    "- High usage/downloads\n",
    "- Few maintainers (‚â§3 ideal, or high dominance)\n",
    "- Critical infrastructure packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Import from centralized candidate lists\nimport sys\nsys.path.insert(0, '../data')\nfrom candidates import (\n    STADIUM_ALL, STADIUM_COLLECTED, STADIUM_HIGH_PRIORITY,\n    FEDERATION_CANDIDATES, CLUB_CANDIDATES, TOY_CANDIDATES,\n    get_uncollected, print_status\n)\n\n# Show collection status across all categories\nprint_status()\n\n# Use the centralized Stadium candidates\nall_candidates = [{\"repo\": repo, \"ecosystem\": \"mixed\"} for repo in STADIUM_ALL]\n\nprint(f\"\\nTotal Stadium candidates: {len(all_candidates)}\")\nprint(f\"Already collected: {len(STADIUM_COLLECTED)}\")\nprint(f\"Remaining: {len(get_uncollected('stadium'))}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Quick Verification - Check Maintainer Counts\n",
    "\n",
    "Before full collection, quickly verify candidates meet Stadium criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quick_verify(repo_name: str) -> dict:\n",
    "    \"\"\"Quick verification of Stadium criteria.\"\"\"\n",
    "    try:\n",
    "        # Get basic metrics\n",
    "        metrics = collector.collect_repository_metrics(repo_name)\n",
    "        maintainers = collector.collect_maintainer_data(repo_name)\n",
    "        \n",
    "        # Get top contributor dominance\n",
    "        contributors = collector.collect_contributor_data(repo_name, max_contributors=10)\n",
    "        \n",
    "        dominance = 0\n",
    "        if contributors:\n",
    "            total = sum(c['contributions'] for c in contributors)\n",
    "            if total > 0:\n",
    "                dominance = contributors[0]['contributions'] / total * 100\n",
    "        \n",
    "        return {\n",
    "            \"repo\": repo_name,\n",
    "            \"stars\": metrics.get('stargazers_count', 0),\n",
    "            \"language\": metrics.get('language', 'Unknown'),\n",
    "            \"active_maintainers\": maintainers['statistics'].get('active_maintainers_6mo', 0),\n",
    "            \"top_contributor\": contributors[0]['login'] if contributors else 'N/A',\n",
    "            \"top_contributor_pct\": round(dominance, 1),\n",
    "            \"stadium_likely\": maintainers['statistics'].get('active_maintainers_6mo', 0) <= 3 or dominance > 40,\n",
    "            \"error\": None\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"repo\": repo_name,\n",
    "            \"error\": str(e)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick verify all candidates (uses ~50-100 API calls per repo)\n",
    "print(\"Quick verification of Stadium candidates...\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "verification_results = []\n",
    "\n",
    "for i, candidate in enumerate(all_candidates):\n",
    "    repo = candidate['repo']\n",
    "    print(f\"[{i+1}/{len(all_candidates)}] Checking {repo}...\", end=\" \")\n",
    "    \n",
    "    result = quick_verify(repo)\n",
    "    result['ecosystem'] = candidate['ecosystem']\n",
    "    verification_results.append(result)\n",
    "    \n",
    "    if result.get('error'):\n",
    "        print(f\"‚ùå Error: {result['error'][:50]}\")\n",
    "    elif result.get('stadium_likely'):\n",
    "        print(f\"‚úÖ Stadium likely ({result['active_maintainers']} maintainers, {result['top_contributor_pct']}% dominance)\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è  Maybe not Stadium ({result['active_maintainers']} maintainers, {result['top_contributor_pct']}% dominance)\")\n",
    "    \n",
    "    # Rate limit check\n",
    "    if (i + 1) % 5 == 0:\n",
    "        rate = collector.get_rate_limit()\n",
    "        print(f\"    [Rate limit: {rate['core']['remaining']}/{rate['core']['limit']}]\")\n",
    "        if rate['core']['remaining'] < 500:\n",
    "            print(\"‚ö†Ô∏è  Rate limit low, pausing...\")\n",
    "            time.sleep(60)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"Verification complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display verification results\n",
    "df_verify = pd.DataFrame(verification_results)\n",
    "\n",
    "# Filter successful verifications\n",
    "df_success = df_verify[df_verify['error'].isna()].copy()\n",
    "\n",
    "print(f\"\\nSuccessfully verified: {len(df_success)}/{len(df_verify)}\")\n",
    "print(f\"Stadium likely: {df_success['stadium_likely'].sum()}\")\n",
    "print(f\"Uncertain: {(~df_success['stadium_likely']).sum()}\")\n",
    "\n",
    "# Display table\n",
    "display_cols = ['repo', 'ecosystem', 'stars', 'active_maintainers', 'top_contributor', 'top_contributor_pct', 'stadium_likely']\n",
    "df_success[display_cols].sort_values('stadium_likely', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Select Confirmed Stadium Projects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Use centralized uncollected list\n# This uses the COLLECTED list from stadium_candidates.py\nuncollected = get_uncollected(\"stadium\")\n\n# Also allow collecting high priority first\nconfirmed_stadium = STADIUM_HIGH_PRIORITY + [r for r in uncollected if r not in STADIUM_HIGH_PRIORITY]\n\nprint(f\"Confirmed Stadium projects to collect ({len(confirmed_stadium)}):\")\nfor repo in confirmed_stadium[:10]:  # Show first 10\n    priority = \"HIGH\" if repo in STADIUM_HIGH_PRIORITY else \"\"\n    print(f\"  - {repo} {priority}\")\nif len(confirmed_stadium) > 10:\n    print(f\"  ... and {len(confirmed_stadium) - 10} more\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Batch Collection - Full Dataset\n",
    "\n",
    "Collect complete data for confirmed Stadium projects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_with_retry(repo_name: str, since_days: int = 365, max_retries: int = 3) -> dict:\n",
    "    \"\"\"Collect data with retry logic.\"\"\"\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            data = collector.collect_complete_dataset(repo_name, since_days=since_days)\n",
    "            return {\"success\": True, \"data\": data, \"error\": None}\n",
    "        except Exception as e:\n",
    "            if \"rate limit\" in str(e).lower():\n",
    "                print(f\"      Rate limit hit, waiting 60s...\")\n",
    "                time.sleep(60)\n",
    "            elif attempt < max_retries - 1:\n",
    "                print(f\"      Retry {attempt + 1}/{max_retries}...\")\n",
    "                time.sleep(5)\n",
    "            else:\n",
    "                return {\"success\": False, \"data\": None, \"error\": str(e)}\n",
    "    return {\"success\": False, \"data\": None, \"error\": \"Max retries exceeded\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check which projects already have data\n",
    "data_dir = Path(\"../data/raw\")\n",
    "existing_files = {f.stem.replace('_data', '').replace('_', '/'): f for f in data_dir.glob(\"*_data.json\")}\n",
    "\n",
    "print(f\"Existing data files: {len(existing_files)}\")\n",
    "for repo in existing_files:\n",
    "    print(f\"  ‚úì {repo}\")\n",
    "\n",
    "# Filter to only collect missing ones\n",
    "to_collect = [repo for repo in confirmed_stadium if repo not in existing_files]\n",
    "print(f\"\\nNeed to collect: {len(to_collect)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch collection\n",
    "collection_results = []\n",
    "start_time = datetime.now()\n",
    "\n",
    "print(f\"Starting batch collection for {len(to_collect)} projects...\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for i, repo in enumerate(to_collect):\n",
    "    print(f\"\\n[{i+1}/{len(to_collect)}] Collecting {repo}...\")\n",
    "    \n",
    "    # Check rate limit before starting\n",
    "    rate = collector.get_rate_limit()\n",
    "    if rate['core']['remaining'] < 500:\n",
    "        wait_time = 60\n",
    "        print(f\"    ‚è≥ Rate limit low ({rate['core']['remaining']}), waiting {wait_time}s...\")\n",
    "        time.sleep(wait_time)\n",
    "    \n",
    "    # Collect data\n",
    "    result = collect_with_retry(repo, since_days=365)\n",
    "    \n",
    "    if result['success']:\n",
    "        # Save to file\n",
    "        output_path = data_dir / f\"{repo.replace('/', '_')}_data.json\"\n",
    "        collector.save_data(result['data'], output_path)\n",
    "        \n",
    "        stars = result['data']['repository'].get('stargazers_count', 0)\n",
    "        contributors = len(result['data']['contributors'])\n",
    "        print(f\"    ‚úÖ Success! ({stars:,} stars, {contributors} contributors)\")\n",
    "        \n",
    "        collection_results.append({\n",
    "            \"repo\": repo,\n",
    "            \"success\": True,\n",
    "            \"stars\": stars,\n",
    "            \"contributors\": contributors,\n",
    "            \"file\": str(output_path)\n",
    "        })\n",
    "    else:\n",
    "        print(f\"    ‚ùå Failed: {result['error'][:50]}\")\n",
    "        collection_results.append({\n",
    "            \"repo\": repo,\n",
    "            \"success\": False,\n",
    "            \"error\": result['error']\n",
    "        })\n",
    "\n",
    "elapsed = datetime.now() - start_time\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(f\"Batch collection complete! Time: {elapsed}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary\n",
    "df_results = pd.DataFrame(collection_results)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"COLLECTION SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if len(df_results) > 0:\n",
    "    success_count = df_results['success'].sum()\n",
    "    print(f\"Successful: {success_count}/{len(df_results)}\")\n",
    "    print(f\"Failed: {len(df_results) - success_count}\")\n",
    "    \n",
    "    if 'stars' in df_results.columns:\n",
    "        df_success = df_results[df_results['success']]\n",
    "        print(f\"\\nTotal stars collected: {df_success['stars'].sum():,}\")\n",
    "        print(f\"Total contributors: {df_success['contributors'].sum():,}\")\n",
    "\n",
    "# Show all collected data\n",
    "all_data_files = list(data_dir.glob(\"*_data.json\"))\n",
    "print(f\"\\nTotal data files: {len(all_data_files)}\")\n",
    "total_size = sum(f.stat().st_size for f in all_data_files) / 1024\n",
    "print(f\"Total size: {total_size:.1f} KB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Final Rate Limit Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rate = collector.get_rate_limit()\n",
    "print(f\"\\nüìä Final Rate Limit Status:\")\n",
    "print(f\"   Core API: {rate['core']['remaining']}/{rate['core']['limit']} remaining\")\n",
    "print(f\"   Resets at: {rate['core']['reset']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "1. Run `01_data_exploration.ipynb` to analyze collected data\n",
    "2. Run `03_statistical_analysis.ipynb` for hypothesis testing\n",
    "3. Add Federation/Club control projects for comparison"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Categories)",
   "language": "python",
   "name": "categories-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}