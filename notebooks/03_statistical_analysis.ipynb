{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03 - Statistical Analysis & Hypothesis Testing\n",
    "\n",
    "Rigorous statistical analysis of Stadium vs Federation/Club projects.\n",
    "\n",
    "**Research Hypotheses:**\n",
    "- **H1**: Stadium projects exhibit significantly lower contributor entropy than Federation projects\n",
    "- **H2**: Stadium projects show higher Gini coefficients (contribution inequality)\n",
    "- **H3**: Stadium entropy correlates with VSM S2 (coordination) metrics\n",
    "- **H4**: Stadium projects have faster PR merge times (centralized decision-making)\n",
    "- **H5**: Stadium projects have fewer governance files\n",
    "- **H6**: Entropy predicts project classification with >80% accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from scipy.stats import mannwhitneyu, ttest_ind, pearsonr, spearmanr\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.power import TTestIndPower\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Add src to path\n",
    "sys.path.insert(0, '../src')\n",
    "from analysis.entropy_calculation import EntropyCalculator\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = [12, 6]\n",
    "\n",
    "print(\"‚úÖ Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all collected project data\n",
    "data_dir = Path(\"../data/raw\")\n",
    "data_files = list(data_dir.glob(\"*_data.json\"))\n",
    "\n",
    "print(f\"Found {len(data_files)} data file(s)\")\n",
    "\n",
    "projects = []\n",
    "entropy_calc = EntropyCalculator()\n",
    "\n",
    "for file_path in data_files:\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    repo = data['repository']\n",
    "    contributors = data.get('contributors', [])\n",
    "    maintainers = data.get('maintainers', {}).get('statistics', {})\n",
    "    pr_stats = data.get('pull_requests', {}).get('statistics', {})\n",
    "    issue_stats = data.get('issues', {}).get('statistics', {})\n",
    "    gov_files = data.get('governance_files', {})\n",
    "    \n",
    "    # Calculate entropy metrics\n",
    "    if contributors:\n",
    "        entropy, normalized_entropy = entropy_calc.contributor_entropy(contributors)\n",
    "        contributions = [c.get('contributions', 0) for c in contributors]\n",
    "        gini = entropy_calc.gini_coefficient(contributions)\n",
    "        \n",
    "        total_contrib = sum(contributions)\n",
    "        top_pct = contributions[0] / total_contrib * 100 if total_contrib > 0 else 0\n",
    "        top_2_pct = sum(contributions[:2]) / total_contrib * 100 if total_contrib > 0 else 0\n",
    "    else:\n",
    "        entropy, normalized_entropy, gini = 0, 0, 0\n",
    "        top_pct, top_2_pct = 0, 0\n",
    "    \n",
    "    # Count governance files\n",
    "    gov_count = sum(1 for v in gov_files.values() if v)\n",
    "    \n",
    "    projects.append({\n",
    "        'repository': repo.get('full_name', 'Unknown'),\n",
    "        'language': repo.get('language', 'Unknown'),\n",
    "        'stars': repo.get('stargazers_count', 0),\n",
    "        'forks': repo.get('forks_count', 0),\n",
    "        'contributors': len(contributors),\n",
    "        'active_maintainers': maintainers.get('active_maintainers_6mo', 0),\n",
    "        'commits': len(data.get('recent_commits', [])),\n",
    "        \n",
    "        # Entropy metrics\n",
    "        'entropy': entropy,\n",
    "        'normalized_entropy': normalized_entropy,\n",
    "        'gini': gini,\n",
    "        'top_contributor_pct': top_pct,\n",
    "        'top_2_pct': top_2_pct,\n",
    "        \n",
    "        # PR metrics (VSM S2 - Coordination)\n",
    "        'total_prs': pr_stats.get('total_prs', 0),\n",
    "        'merge_rate': pr_stats.get('merged_count', 0) / max(pr_stats.get('total_prs', 1), 1),\n",
    "        'avg_merge_time': pr_stats.get('avg_time_to_merge', 0),\n",
    "        'conflict_rate': pr_stats.get('conflict_rate', 0),\n",
    "        \n",
    "        # Issue metrics (VSM S1 - Operations)\n",
    "        'total_issues': issue_stats.get('total_issues', 0),\n",
    "        'avg_close_time': issue_stats.get('avg_time_to_close', 0),\n",
    "        \n",
    "        # Governance\n",
    "        'governance_files': gov_count,\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(projects)\n",
    "print(f\"\\nLoaded {len(df)} projects\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Classify Projects\n",
    "\n",
    "Classify based on entropy and dominance metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_project(row):\n",
    "    \"\"\"Classify project as Stadium, Federation, or Club.\"\"\"\n",
    "    \n",
    "    # Stadium indicators\n",
    "    low_maintainers = row['active_maintainers'] <= 3\n",
    "    low_entropy = row['normalized_entropy'] < 0.5\n",
    "    high_dominance = row['top_contributor_pct'] > 40\n",
    "    high_gini = row['gini'] > 0.7\n",
    "    \n",
    "    # Federation indicators  \n",
    "    many_maintainers = row['active_maintainers'] > 5\n",
    "    high_entropy = row['normalized_entropy'] > 0.6\n",
    "    distributed = row['top_contributor_pct'] < 25\n",
    "    \n",
    "    # Score-based classification\n",
    "    stadium_score = sum([low_maintainers, low_entropy, high_dominance, high_gini])\n",
    "    federation_score = sum([many_maintainers, high_entropy, distributed])\n",
    "    \n",
    "    if stadium_score >= 3:\n",
    "        return 'Stadium'\n",
    "    elif stadium_score >= 2 and federation_score < 2:\n",
    "        return 'Stadium'\n",
    "    elif federation_score >= 2:\n",
    "        return 'Federation'\n",
    "    else:\n",
    "        return 'Club'  # Hybrid/uncertain\n",
    "\n",
    "df['classification'] = df.apply(classify_project, axis=1)\n",
    "\n",
    "print(\"Project Classifications:\")\n",
    "print(df['classification'].value_counts())\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "df[['repository', 'active_maintainers', 'normalized_entropy', 'top_contributor_pct', 'gini', 'classification']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Power Analysis\n",
    "\n",
    "Determine if we have sufficient sample size for meaningful results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Power analysis for two-sample t-test\n",
    "power_analysis = TTestIndPower()\n",
    "\n",
    "# Expected effect size (Cohen's d)\n",
    "effect_sizes = [0.5, 0.8, 1.0, 1.2]  # medium to large\n",
    "alpha = 0.05\n",
    "power = 0.80\n",
    "\n",
    "print(\"Required Sample Sizes (per group) for 80% Power:\")\n",
    "print(\"=\"*50)\n",
    "for d in effect_sizes:\n",
    "    n = power_analysis.solve_power(effect_size=d, alpha=alpha, power=power, alternative='two-sided')\n",
    "    print(f\"  Cohen's d = {d}: n = {int(np.ceil(n))} per group\")\n",
    "\n",
    "# Calculate achieved power with current sample\n",
    "n_stadium = len(df[df['classification'] == 'Stadium'])\n",
    "n_other = len(df[df['classification'] != 'Stadium'])\n",
    "n_min = min(n_stadium, n_other)\n",
    "\n",
    "print(f\"\\nCurrent Sample:\")\n",
    "print(f\"  Stadium: {n_stadium}\")\n",
    "print(f\"  Other: {n_other}\")\n",
    "\n",
    "if n_min > 2:\n",
    "    for d in effect_sizes:\n",
    "        achieved_power = power_analysis.solve_power(effect_size=d, nobs1=n_min, alpha=alpha, alternative='two-sided')\n",
    "        print(f\"\\nAchieved power for d={d}: {achieved_power:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Hypothesis Testing\n",
    "\n",
    "### H1: Stadium projects have lower contributor entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split groups\n",
    "stadium = df[df['classification'] == 'Stadium']['normalized_entropy']\n",
    "non_stadium = df[df['classification'] != 'Stadium']['normalized_entropy']\n",
    "\n",
    "print(\"H1: Stadium projects have lower contributor entropy\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if len(stadium) >= 2 and len(non_stadium) >= 2:\n",
    "    # Descriptive statistics\n",
    "    print(f\"\\nDescriptive Statistics:\")\n",
    "    print(f\"  Stadium (n={len(stadium)}):     mean={stadium.mean():.3f}, std={stadium.std():.3f}\")\n",
    "    print(f\"  Non-Stadium (n={len(non_stadium)}): mean={non_stadium.mean():.3f}, std={non_stadium.std():.3f}\")\n",
    "    \n",
    "    # Effect size (Cohen's d)\n",
    "    pooled_std = np.sqrt(((len(stadium)-1)*stadium.std()**2 + (len(non_stadium)-1)*non_stadium.std()**2) / \n",
    "                         (len(stadium) + len(non_stadium) - 2))\n",
    "    cohens_d = (non_stadium.mean() - stadium.mean()) / pooled_std if pooled_std > 0 else 0\n",
    "    print(f\"\\n  Cohen's d: {cohens_d:.3f} ({'large' if abs(cohens_d) > 0.8 else 'medium' if abs(cohens_d) > 0.5 else 'small'})\")\n",
    "    \n",
    "    # Mann-Whitney U test (non-parametric)\n",
    "    stat, p_value = mannwhitneyu(stadium, non_stadium, alternative='less')\n",
    "    print(f\"\\nMann-Whitney U Test:\")\n",
    "    print(f\"  U-statistic: {stat:.2f}\")\n",
    "    print(f\"  p-value: {p_value:.4f}\")\n",
    "    print(f\"  Result: {'‚úÖ SIGNIFICANT' if p_value < 0.05 else '‚ùå Not significant'} (Œ±=0.05)\")\n",
    "    \n",
    "    # t-test (parametric)\n",
    "    t_stat, t_p = ttest_ind(stadium, non_stadium, alternative='less')\n",
    "    print(f\"\\nt-test (for reference):\")\n",
    "    print(f\"  t-statistic: {t_stat:.2f}\")\n",
    "    print(f\"  p-value: {t_p:.4f}\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  Insufficient data for statistical testing\")\n",
    "    print(f\"    Stadium: {len(stadium)}, Non-Stadium: {len(non_stadium)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### H2: Stadium projects have higher Gini coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stadium_gini = df[df['classification'] == 'Stadium']['gini']\n",
    "non_stadium_gini = df[df['classification'] != 'Stadium']['gini']\n",
    "\n",
    "print(\"H2: Stadium projects have higher Gini coefficients\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if len(stadium_gini) >= 2 and len(non_stadium_gini) >= 2:\n",
    "    print(f\"\\nDescriptive Statistics:\")\n",
    "    print(f\"  Stadium (n={len(stadium_gini)}):     mean={stadium_gini.mean():.3f}, std={stadium_gini.std():.3f}\")\n",
    "    print(f\"  Non-Stadium (n={len(non_stadium_gini)}): mean={non_stadium_gini.mean():.3f}, std={non_stadium_gini.std():.3f}\")\n",
    "    \n",
    "    stat, p_value = mannwhitneyu(stadium_gini, non_stadium_gini, alternative='greater')\n",
    "    print(f\"\\nMann-Whitney U Test:\")\n",
    "    print(f\"  U-statistic: {stat:.2f}\")\n",
    "    print(f\"  p-value: {p_value:.4f}\")\n",
    "    print(f\"  Result: {'‚úÖ SIGNIFICANT' if p_value < 0.05 else '‚ùå Not significant'} (Œ±=0.05)\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  Insufficient data for statistical testing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### H3: Entropy correlates with VSM S2 (Coordination) metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"H3: Entropy correlates with coordination metrics\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Correlation between entropy and conflict rate (VSM S2 indicator)\n",
    "valid_data = df[df['conflict_rate'] > 0]\n",
    "\n",
    "if len(valid_data) >= 3:\n",
    "    # Pearson correlation\n",
    "    r, p = pearsonr(valid_data['normalized_entropy'], valid_data['conflict_rate'])\n",
    "    print(f\"\\nEntropy vs Conflict Rate:\")\n",
    "    print(f\"  Pearson r: {r:.3f}\")\n",
    "    print(f\"  p-value: {p:.4f}\")\n",
    "    print(f\"  Result: {'‚úÖ SIGNIFICANT' if p < 0.05 else '‚ùå Not significant'}\")\n",
    "    \n",
    "    # Spearman correlation (non-parametric)\n",
    "    rho, p_spearman = spearmanr(valid_data['normalized_entropy'], valid_data['conflict_rate'])\n",
    "    print(f\"\\n  Spearman œÅ: {rho:.3f} (p={p_spearman:.4f})\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  Insufficient data with conflict rates\")\n",
    "\n",
    "# Correlation with merge time\n",
    "valid_merge = df[df['avg_merge_time'] > 0]\n",
    "if len(valid_merge) >= 3:\n",
    "    r, p = pearsonr(valid_merge['normalized_entropy'], valid_merge['avg_merge_time'])\n",
    "    print(f\"\\nEntropy vs Merge Time:\")\n",
    "    print(f\"  Pearson r: {r:.3f}\")\n",
    "    print(f\"  p-value: {p:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### H4: Stadium projects have faster PR merge times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stadium_merge = df[(df['classification'] == 'Stadium') & (df['avg_merge_time'] > 0)]['avg_merge_time']\n",
    "non_stadium_merge = df[(df['classification'] != 'Stadium') & (df['avg_merge_time'] > 0)]['avg_merge_time']\n",
    "\n",
    "print(\"H4: Stadium projects have faster PR merge times\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if len(stadium_merge) >= 2 and len(non_stadium_merge) >= 2:\n",
    "    print(f\"\\nDescriptive Statistics (hours):\")\n",
    "    print(f\"  Stadium (n={len(stadium_merge)}):     median={stadium_merge.median():.1f}, mean={stadium_merge.mean():.1f}\")\n",
    "    print(f\"  Non-Stadium (n={len(non_stadium_merge)}): median={non_stadium_merge.median():.1f}, mean={non_stadium_merge.mean():.1f}\")\n",
    "    \n",
    "    stat, p_value = mannwhitneyu(stadium_merge, non_stadium_merge, alternative='less')\n",
    "    print(f\"\\nMann-Whitney U Test:\")\n",
    "    print(f\"  p-value: {p_value:.4f}\")\n",
    "    print(f\"  Result: {'‚úÖ SIGNIFICANT' if p_value < 0.05 else '‚ùå Not significant'}\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  Insufficient data for statistical testing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### H5: Stadium projects have fewer governance files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stadium_gov = df[df['classification'] == 'Stadium']['governance_files']\n",
    "non_stadium_gov = df[df['classification'] != 'Stadium']['governance_files']\n",
    "\n",
    "print(\"H5: Stadium projects have fewer governance files\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if len(stadium_gov) >= 2 and len(non_stadium_gov) >= 2:\n",
    "    print(f\"\\nDescriptive Statistics:\")\n",
    "    print(f\"  Stadium (n={len(stadium_gov)}):     mean={stadium_gov.mean():.2f}, median={stadium_gov.median():.0f}\")\n",
    "    print(f\"  Non-Stadium (n={len(non_stadium_gov)}): mean={non_stadium_gov.mean():.2f}, median={non_stadium_gov.median():.0f}\")\n",
    "    \n",
    "    stat, p_value = mannwhitneyu(stadium_gov, non_stadium_gov, alternative='less')\n",
    "    print(f\"\\nMann-Whitney U Test:\")\n",
    "    print(f\"  p-value: {p_value:.4f}\")\n",
    "    print(f\"  Result: {'‚úÖ SIGNIFICANT' if p_value < 0.05 else '‚ùå Not significant'}\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  Insufficient data for statistical testing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### H6: Entropy predicts classification with >80% accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"H6: Entropy predicts classification with >80% accuracy\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Prepare features and target\n",
    "feature_cols = ['normalized_entropy', 'gini', 'top_contributor_pct']\n",
    "X = df[feature_cols].values\n",
    "y = (df['classification'] == 'Stadium').astype(int).values\n",
    "\n",
    "if len(df) >= 10 and y.sum() >= 2 and (len(y) - y.sum()) >= 2:\n",
    "    # Logistic regression with cross-validation\n",
    "    model = LogisticRegression(random_state=42, max_iter=1000)\n",
    "    \n",
    "    # Cross-validation\n",
    "    cv_scores = cross_val_score(model, X, y, cv=min(5, len(df)//2), scoring='accuracy')\n",
    "    \n",
    "    print(f\"\\nLogistic Regression (features: {feature_cols})\")\n",
    "    print(f\"  Cross-validation accuracy: {cv_scores.mean():.1%} (¬±{cv_scores.std():.1%})\")\n",
    "    print(f\"  Result: {'‚úÖ >80% ACHIEVED' if cv_scores.mean() > 0.80 else '‚ùå <80%'}\")\n",
    "    \n",
    "    # Fit on all data for coefficients\n",
    "    model.fit(X, y)\n",
    "    print(f\"\\nFeature Importance (coefficients):\")\n",
    "    for feat, coef in zip(feature_cols, model.coef_[0]):\n",
    "        print(f\"  {feat}: {coef:.3f}\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  Insufficient data for classification model\")\n",
    "    print(f\"    Total samples: {len(df)}, Stadium: {y.sum()}, Other: {len(y) - y.sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(df) >= 2:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    \n",
    "    # 1. Entropy distribution by classification\n",
    "    ax1 = axes[0, 0]\n",
    "    for cls in df['classification'].unique():\n",
    "        data = df[df['classification'] == cls]['normalized_entropy']\n",
    "        ax1.hist(data, alpha=0.5, label=f\"{cls} (n={len(data)})\", bins=10)\n",
    "    ax1.set_xlabel('Normalized Entropy')\n",
    "    ax1.set_ylabel('Count')\n",
    "    ax1.set_title('Entropy Distribution by Classification')\n",
    "    ax1.legend()\n",
    "    \n",
    "    # 2. Entropy vs Gini scatter\n",
    "    ax2 = axes[0, 1]\n",
    "    for cls in df['classification'].unique():\n",
    "        subset = df[df['classification'] == cls]\n",
    "        ax2.scatter(subset['normalized_entropy'], subset['gini'], \n",
    "                   label=cls, s=100, alpha=0.7)\n",
    "    ax2.set_xlabel('Normalized Entropy')\n",
    "    ax2.set_ylabel('Gini Coefficient')\n",
    "    ax2.set_title('Entropy vs Gini by Classification')\n",
    "    ax2.legend()\n",
    "    \n",
    "    # 3. Top contributor dominance\n",
    "    ax3 = axes[1, 0]\n",
    "    df_sorted = df.sort_values('top_contributor_pct', ascending=True)\n",
    "    colors = ['coral' if c == 'Stadium' else 'steelblue' for c in df_sorted['classification']]\n",
    "    ax3.barh(range(len(df_sorted)), df_sorted['top_contributor_pct'], color=colors)\n",
    "    ax3.set_yticks(range(len(df_sorted)))\n",
    "    ax3.set_yticklabels([r[:20] for r in df_sorted['repository']])\n",
    "    ax3.set_xlabel('Top Contributor %')\n",
    "    ax3.set_title('Top Contributor Dominance (orange=Stadium)')\n",
    "    ax3.axvline(x=40, color='red', linestyle='--', alpha=0.5, label='40% threshold')\n",
    "    \n",
    "    # 4. Summary metrics comparison\n",
    "    ax4 = axes[1, 1]\n",
    "    metrics = ['normalized_entropy', 'gini', 'active_maintainers']\n",
    "    x = np.arange(len(metrics))\n",
    "    width = 0.35\n",
    "    \n",
    "    stadium_means = df[df['classification'] == 'Stadium'][metrics].mean()\n",
    "    other_means = df[df['classification'] != 'Stadium'][metrics].mean()\n",
    "    \n",
    "    # Normalize for comparison\n",
    "    max_vals = df[metrics].max()\n",
    "    stadium_norm = stadium_means / max_vals\n",
    "    other_norm = other_means / max_vals\n",
    "    \n",
    "    ax4.bar(x - width/2, stadium_norm, width, label='Stadium', color='coral')\n",
    "    ax4.bar(x + width/2, other_norm, width, label='Other', color='steelblue')\n",
    "    ax4.set_xticks(x)\n",
    "    ax4.set_xticklabels(['Entropy', 'Gini', 'Maintainers'])\n",
    "    ax4.set_ylabel('Normalized Value')\n",
    "    ax4.set_title('Key Metrics Comparison')\n",
    "    ax4.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../data/processed/statistical_analysis.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(\"\\nüìä Figure saved to data/processed/statistical_analysis.png\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Need more data for visualizations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"STATISTICAL ANALYSIS SUMMARY REPORT\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nDataset:\")\n",
    "print(f\"  Total projects: {len(df)}\")\n",
    "for cls in df['classification'].value_counts().items():\n",
    "    print(f\"  {cls[0]}: {cls[1]}\")\n",
    "\n",
    "print(f\"\\nKey Findings:\")\n",
    "print(f\"  Stadium normalized entropy: {df[df['classification']=='Stadium']['normalized_entropy'].mean():.3f} (mean)\")\n",
    "print(f\"  Non-Stadium normalized entropy: {df[df['classification']!='Stadium']['normalized_entropy'].mean():.3f} (mean)\")\n",
    "print(f\"  Stadium Gini coefficient: {df[df['classification']=='Stadium']['gini'].mean():.3f} (mean)\")\n",
    "print(f\"  Stadium top contributor %: {df[df['classification']=='Stadium']['top_contributor_pct'].mean():.1f}% (mean)\")\n",
    "\n",
    "print(f\"\\nHypothesis Results:\")\n",
    "print(f\"  H1 (Lower entropy): {'Needs more data' if len(stadium) < 2 else 'See above'}\")\n",
    "print(f\"  H2 (Higher Gini): {'Needs more data' if len(stadium) < 2 else 'See above'}\")\n",
    "print(f\"  H3 (Correlation): {'Needs more data' if len(valid_data) < 3 else 'See above'}\")\n",
    "print(f\"  H4 (Faster merge): {'Needs more data' if len(stadium_merge) < 2 else 'See above'}\")\n",
    "print(f\"  H5 (Fewer gov files): {'Needs more data' if len(stadium_gov) < 2 else 'See above'}\")\n",
    "print(f\"  H6 (>80% accuracy): {'Needs more data' if len(df) < 10 else 'See above'}\")\n",
    "\n",
    "print(f\"\\nRecommendations:\")\n",
    "if len(df) < 30:\n",
    "    print(f\"  ‚ö†Ô∏è  Collect more Stadium projects (target: 28-30)\")\n",
    "    print(f\"     Current: {len(df[df['classification']=='Stadium'])} Stadium projects\")\n",
    "if len(df[df['classification'] != 'Stadium']) < 15:\n",
    "    print(f\"  ‚ö†Ô∏è  Need Federation/Club control projects for comparison\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export results\n",
    "output_dir = Path(\"../data/processed\")\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save analysis DataFrame\n",
    "df.to_csv(output_dir / \"statistical_analysis_results.csv\", index=False)\n",
    "print(f\"\\n‚úÖ Results saved to {output_dir / 'statistical_analysis_results.csv'}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Categories)",
   "language": "python",
   "name": "categories-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
