{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# 06 - Temporal Analysis: Entropy Evolution Over Time\n",
    "\n",
    "Analyze how project entropy and governance characteristics change over the project lifecycle.\n",
    "\n",
    "**Goals:**\n",
    "1. Track entropy evolution over commit history\n",
    "2. Identify governance transition points\n",
    "3. Analyze contributor churn and its effects\n",
    "4. Predict sustainability based on temporal patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from scipy.signal import savgol_filter\n",
    "\n",
    "# Add src to path\n",
    "sys.path.insert(0, '../src')\n",
    "from analysis.entropy_calculation import EntropyCalculator\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = [14, 8]\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "# Initialize entropy calculator\n",
    "entropy_calc = EntropyCalculator()\n",
    "\n",
    "print(\"âœ… Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "## 1. Load Project Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all collected project data\n",
    "data_dir = Path(\"../data/raw\")\n",
    "projects = {}\n",
    "\n",
    "for file_path in data_dir.glob(\"*_data.json\"):\n",
    "    with open(file_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "        repo_name = data['repository']['full_name']\n",
    "        projects[repo_name] = data\n",
    "\n",
    "print(f\"Loaded {len(projects)} projects:\")\n",
    "for name, data in projects.items():\n",
    "    commits = len(data.get('recent_commits', []))\n",
    "    print(f\"  - {name}: {commits} commits\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## 2. Temporal Entropy Calculation\n",
    "\n",
    "Calculate entropy using sliding windows over commit history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_commit_date(date_str: str) -> datetime:\n",
    "    \"\"\"Parse commit date string to datetime.\"\"\"\n",
    "    try:\n",
    "        return datetime.fromisoformat(date_str.replace('Z', '+00:00'))\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def calculate_window_entropy(commits: list, window_days: int = 30) -> list:\n",
    "    \"\"\"\n",
    "    Calculate entropy using sliding windows.\n",
    "    \n",
    "    Args:\n",
    "        commits: List of commit dictionaries with 'date' and 'author'\n",
    "        window_days: Size of sliding window in days\n",
    "    \n",
    "    Returns:\n",
    "        List of (date, entropy, commit_count, unique_authors) tuples\n",
    "    \"\"\"\n",
    "    if not commits:\n",
    "        return []\n",
    "    \n",
    "    # Parse dates and filter valid commits\n",
    "    dated_commits = []\n",
    "    for c in commits:\n",
    "        date = parse_commit_date(c.get('date', ''))\n",
    "        if date and c.get('author'):\n",
    "            dated_commits.append({'date': date, 'author': c['author']})\n",
    "    \n",
    "    if not dated_commits:\n",
    "        return []\n",
    "    \n",
    "    # Sort by date\n",
    "    dated_commits.sort(key=lambda x: x['date'])\n",
    "    \n",
    "    # Get date range\n",
    "    start_date = dated_commits[0]['date']\n",
    "    end_date = dated_commits[-1]['date']\n",
    "    \n",
    "    # Calculate entropy for each window\n",
    "    results = []\n",
    "    window = timedelta(days=window_days)\n",
    "    step = timedelta(days=window_days // 4)  # 25% overlap\n",
    "    \n",
    "    current_start = start_date\n",
    "    while current_start < end_date:\n",
    "        current_end = current_start + window\n",
    "        \n",
    "        # Get commits in window\n",
    "        window_commits = [\n",
    "            c for c in dated_commits \n",
    "            if current_start <= c['date'] < current_end\n",
    "        ]\n",
    "        \n",
    "        if len(window_commits) >= 5:  # Minimum commits for meaningful entropy\n",
    "            # Count contributions per author\n",
    "            author_counts = defaultdict(int)\n",
    "            for c in window_commits:\n",
    "                author_counts[c['author']] += 1\n",
    "            \n",
    "            # Calculate entropy\n",
    "            counts = list(author_counts.values())\n",
    "            total = sum(counts)\n",
    "            probs = np.array(counts) / total\n",
    "            entropy = -np.sum(probs * np.log2(probs + 1e-10))\n",
    "            \n",
    "            # Normalize by max possible entropy\n",
    "            max_entropy = np.log2(len(counts))\n",
    "            norm_entropy = entropy / max_entropy if max_entropy > 0 else 0\n",
    "            \n",
    "            # Top contributor dominance\n",
    "            top_dominance = max(counts) / total * 100\n",
    "            \n",
    "            results.append({\n",
    "                'date': current_start + window / 2,  # Window midpoint\n",
    "                'entropy': entropy,\n",
    "                'normalized_entropy': norm_entropy,\n",
    "                'commit_count': len(window_commits),\n",
    "                'unique_authors': len(author_counts),\n",
    "                'top_dominance': top_dominance\n",
    "            })\n",
    "        \n",
    "        current_start += step\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"âœ… Temporal entropy functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## 3. Calculate Temporal Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate temporal entropy for all projects\n",
    "temporal_data = {}\n",
    "\n",
    "for repo_name, data in projects.items():\n",
    "    commits = data.get('recent_commits', [])\n",
    "    \n",
    "    if len(commits) < 20:\n",
    "        print(f\"  âš ï¸  {repo_name}: Insufficient commits ({len(commits)})\")\n",
    "        continue\n",
    "    \n",
    "    # Calculate with 30-day windows\n",
    "    entropy_series = calculate_window_entropy(commits, window_days=30)\n",
    "    \n",
    "    if entropy_series:\n",
    "        temporal_data[repo_name] = pd.DataFrame(entropy_series)\n",
    "        print(f\"  âœ“ {repo_name}: {len(entropy_series)} data points\")\n",
    "    else:\n",
    "        print(f\"  âš ï¸  {repo_name}: Could not calculate entropy series\")\n",
    "\n",
    "print(f\"\\nâœ… Calculated temporal entropy for {len(temporal_data)} projects\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## 4. Entropy Evolution Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_entropy_evolution(df: pd.DataFrame, repo_name: str, ax=None):\n",
    "    \"\"\"\n",
    "    Plot entropy evolution over time with trend analysis.\n",
    "    \"\"\"\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=(14, 6))\n",
    "    \n",
    "    # Convert dates\n",
    "    dates = pd.to_datetime(df['date'])\n",
    "    \n",
    "    # Plot normalized entropy\n",
    "    ax.plot(dates, df['normalized_entropy'], 'b-', alpha=0.5, linewidth=1, label='Raw')\n",
    "    \n",
    "    # Smoothed line\n",
    "    if len(df) >= 5:\n",
    "        window = min(5, len(df) // 2)\n",
    "        if window >= 3:\n",
    "            smoothed = savgol_filter(df['normalized_entropy'], window, 2)\n",
    "            ax.plot(dates, smoothed, 'b-', linewidth=2, label='Smoothed')\n",
    "    \n",
    "    # Linear trend\n",
    "    x_numeric = np.arange(len(df))\n",
    "    slope, intercept, r_value, p_value, std_err = stats.linregress(\n",
    "        x_numeric, df['normalized_entropy']\n",
    "    )\n",
    "    trend_line = slope * x_numeric + intercept\n",
    "    ax.plot(dates, trend_line, 'r--', linewidth=1.5, \n",
    "           label=f'Trend (slope={slope:.4f}, RÂ²={r_value**2:.3f})')\n",
    "    \n",
    "    # Formatting\n",
    "    ax.set_xlabel('Date')\n",
    "    ax.set_ylabel('Normalized Entropy')\n",
    "    ax.set_title(f'{repo_name} - Entropy Evolution', fontsize=12, fontweight='bold')\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.legend(loc='upper right')\n",
    "    ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m'))\n",
    "    ax.xaxis.set_major_locator(mdates.MonthLocator(interval=2))\n",
    "    plt.setp(ax.xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "    \n",
    "    # Add interpretation\n",
    "    if slope > 0.01:\n",
    "        interpretation = \"â†‘ Increasing distribution\"\n",
    "        color = 'green'\n",
    "    elif slope < -0.01:\n",
    "        interpretation = \"â†“ Increasing concentration\"\n",
    "        color = 'red'\n",
    "    else:\n",
    "        interpretation = \"â†’ Stable governance\"\n",
    "        color = 'blue'\n",
    "    \n",
    "    ax.text(0.02, 0.95, interpretation, transform=ax.transAxes, \n",
    "           fontsize=10, color=color, fontweight='bold', va='top')\n",
    "    \n",
    "    return ax\n",
    "\n",
    "# Plot for all projects\n",
    "if temporal_data:\n",
    "    n_projects = len(temporal_data)\n",
    "    fig, axes = plt.subplots(n_projects, 1, figsize=(14, 5 * n_projects))\n",
    "    \n",
    "    if n_projects == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for ax, (repo_name, df) in zip(axes, temporal_data.items()):\n",
    "        plot_entropy_evolution(df, repo_name, ax)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../docs/diagrams/temporal_entropy_evolution.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(\"\\nâœ… Saved temporal_entropy_evolution.png\")\n",
    "else:\n",
    "    print(\"No temporal data to plot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "## 5. Contributor Activity Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_contributor_activity(commits: list) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Analyze temporal patterns of contributor activity.\n",
    "    \"\"\"\n",
    "    if not commits:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Parse commits\n",
    "    commit_data = []\n",
    "    for c in commits:\n",
    "        date = parse_commit_date(c.get('date', ''))\n",
    "        if date and c.get('author'):\n",
    "            commit_data.append({\n",
    "                'date': date,\n",
    "                'author': c['author'],\n",
    "                'additions': c.get('additions', 0),\n",
    "                'deletions': c.get('deletions', 0)\n",
    "            })\n",
    "    \n",
    "    if not commit_data:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    df = pd.DataFrame(commit_data)\n",
    "    df['month'] = df['date'].dt.to_period('M')\n",
    "    \n",
    "    # Calculate monthly statistics\n",
    "    monthly = df.groupby('month').agg({\n",
    "        'author': ['count', 'nunique'],\n",
    "        'additions': 'sum',\n",
    "        'deletions': 'sum'\n",
    "    })\n",
    "    monthly.columns = ['commits', 'unique_authors', 'additions', 'deletions']\n",
    "    monthly['churn'] = monthly['additions'] + monthly['deletions']\n",
    "    \n",
    "    return monthly.reset_index()\n",
    "\n",
    "# Analyze contributor activity\n",
    "print(\"Contributor Activity Analysis:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for repo_name, data in projects.items():\n",
    "    commits = data.get('recent_commits', [])\n",
    "    monthly = analyze_contributor_activity(commits)\n",
    "    \n",
    "    if not monthly.empty:\n",
    "        print(f\"\\n{repo_name}:\")\n",
    "        print(f\"  Avg commits/month: {monthly['commits'].mean():.1f}\")\n",
    "        print(f\"  Avg unique authors/month: {monthly['unique_authors'].mean():.1f}\")\n",
    "        print(f\"  Max authors in a month: {monthly['unique_authors'].max()}\")\n",
    "        print(f\"  Total code churn: {monthly['churn'].sum():,} lines\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## 6. Governance Transition Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_transitions(df: pd.DataFrame, threshold: float = 0.15) -> list:\n",
    "    \"\"\"\n",
    "    Detect significant changes in entropy (potential governance transitions).\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with normalized_entropy column\n",
    "        threshold: Minimum change to consider significant\n",
    "    \n",
    "    Returns:\n",
    "        List of transition points with details\n",
    "    \"\"\"\n",
    "    if len(df) < 3:\n",
    "        return []\n",
    "    \n",
    "    transitions = []\n",
    "    entropy = df['normalized_entropy'].values\n",
    "    dates = pd.to_datetime(df['date'])\n",
    "    \n",
    "    # Calculate rolling differences\n",
    "    for i in range(2, len(entropy)):\n",
    "        # Compare to previous period\n",
    "        prev_avg = np.mean(entropy[max(0, i-3):i])\n",
    "        curr_avg = np.mean(entropy[i:min(len(entropy), i+3)])\n",
    "        change = curr_avg - prev_avg\n",
    "        \n",
    "        if abs(change) >= threshold:\n",
    "            transitions.append({\n",
    "                'date': dates.iloc[i],\n",
    "                'change': change,\n",
    "                'direction': 'increasing' if change > 0 else 'decreasing',\n",
    "                'before': prev_avg,\n",
    "                'after': curr_avg,\n",
    "                'interpretation': (\n",
    "                    'Distribution increasing (toward Federation)' if change > 0\n",
    "                    else 'Concentration increasing (toward Stadium)'\n",
    "                )\n",
    "            })\n",
    "    \n",
    "    return transitions\n",
    "\n",
    "# Detect transitions for all projects\n",
    "print(\"\\nGovernance Transition Detection:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for repo_name, df in temporal_data.items():\n",
    "    transitions = detect_transitions(df)\n",
    "    \n",
    "    print(f\"\\n{repo_name}:\")\n",
    "    if transitions:\n",
    "        for t in transitions:\n",
    "            print(f\"  ðŸ“Œ {t['date'].strftime('%Y-%m')}: {t['interpretation']}\")\n",
    "            print(f\"      Change: {t['before']:.3f} â†’ {t['after']:.3f} ({t['change']:+.3f})\")\n",
    "    else:\n",
    "        print(f\"  No significant transitions detected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "## 7. Bus Factor Temporal Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_bus_factor_over_time(commits: list, window_days: int = 90) -> list:\n",
    "    \"\"\"\n",
    "    Calculate bus factor (minimum contributors for 50% of commits) over time.\n",
    "    \"\"\"\n",
    "    if not commits:\n",
    "        return []\n",
    "    \n",
    "    # Parse commits\n",
    "    dated_commits = []\n",
    "    for c in commits:\n",
    "        date = parse_commit_date(c.get('date', ''))\n",
    "        if date and c.get('author'):\n",
    "            dated_commits.append({'date': date, 'author': c['author']})\n",
    "    \n",
    "    if not dated_commits:\n",
    "        return []\n",
    "    \n",
    "    dated_commits.sort(key=lambda x: x['date'])\n",
    "    \n",
    "    results = []\n",
    "    window = timedelta(days=window_days)\n",
    "    step = timedelta(days=window_days // 3)\n",
    "    \n",
    "    start_date = dated_commits[0]['date']\n",
    "    end_date = dated_commits[-1]['date']\n",
    "    \n",
    "    current = start_date\n",
    "    while current < end_date:\n",
    "        window_end = current + window\n",
    "        window_commits = [c for c in dated_commits if current <= c['date'] < window_end]\n",
    "        \n",
    "        if len(window_commits) >= 10:\n",
    "            # Count contributions\n",
    "            author_counts = defaultdict(int)\n",
    "            for c in window_commits:\n",
    "                author_counts[c['author']] += 1\n",
    "            \n",
    "            # Sort by contributions\n",
    "            sorted_authors = sorted(author_counts.values(), reverse=True)\n",
    "            total = sum(sorted_authors)\n",
    "            \n",
    "            # Calculate bus factor (contributors for 50%)\n",
    "            cumsum = 0\n",
    "            bus_factor = 0\n",
    "            for count in sorted_authors:\n",
    "                cumsum += count\n",
    "                bus_factor += 1\n",
    "                if cumsum >= total * 0.5:\n",
    "                    break\n",
    "            \n",
    "            results.append({\n",
    "                'date': current + window / 2,\n",
    "                'bus_factor': bus_factor,\n",
    "                'total_contributors': len(author_counts),\n",
    "                'top_contributor_pct': sorted_authors[0] / total * 100\n",
    "            })\n",
    "        \n",
    "        current += step\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Calculate bus factor over time\n",
    "bus_factor_data = {}\n",
    "\n",
    "for repo_name, data in projects.items():\n",
    "    commits = data.get('recent_commits', [])\n",
    "    bf_series = calculate_bus_factor_over_time(commits)\n",
    "    \n",
    "    if bf_series:\n",
    "        bus_factor_data[repo_name] = pd.DataFrame(bf_series)\n",
    "\n",
    "# Plot bus factor evolution\n",
    "if bus_factor_data:\n",
    "    fig, axes = plt.subplots(len(bus_factor_data), 1, figsize=(12, 4 * len(bus_factor_data)))\n",
    "    \n",
    "    if len(bus_factor_data) == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for ax, (repo_name, df) in zip(axes, bus_factor_data.items()):\n",
    "        dates = pd.to_datetime(df['date'])\n",
    "        \n",
    "        ax.plot(dates, df['bus_factor'], 'r-', linewidth=2, marker='o', markersize=4)\n",
    "        ax.axhline(y=1, color='red', linestyle='--', alpha=0.5, label='Critical (1)')\n",
    "        ax.axhline(y=3, color='orange', linestyle='--', alpha=0.5, label='Warning (3)')\n",
    "        \n",
    "        ax.set_xlabel('Date')\n",
    "        ax.set_ylabel('Bus Factor')\n",
    "        ax.set_title(f'{repo_name} - Bus Factor Over Time', fontweight='bold')\n",
    "        ax.set_ylim(0, max(10, df['bus_factor'].max() + 1))\n",
    "        ax.legend()\n",
    "        ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m'))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../docs/diagrams/bus_factor_temporal.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(\"\\nâœ… Saved bus_factor_temporal.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "## 8. Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate temporal summary\n",
    "summary_data = []\n",
    "\n",
    "for repo_name in temporal_data:\n",
    "    df = temporal_data[repo_name]\n",
    "    \n",
    "    # Trend analysis\n",
    "    x = np.arange(len(df))\n",
    "    slope, intercept, r_value, p_value, std_err = stats.linregress(\n",
    "        x, df['normalized_entropy']\n",
    "    )\n",
    "    \n",
    "    # Bus factor stats\n",
    "    bf_df = bus_factor_data.get(repo_name)\n",
    "    avg_bus_factor = bf_df['bus_factor'].mean() if bf_df is not None else None\n",
    "    \n",
    "    summary_data.append({\n",
    "        'repository': repo_name,\n",
    "        'mean_entropy': df['normalized_entropy'].mean(),\n",
    "        'std_entropy': df['normalized_entropy'].std(),\n",
    "        'trend_slope': slope,\n",
    "        'trend_r2': r_value ** 2,\n",
    "        'trend_significant': p_value < 0.05,\n",
    "        'avg_bus_factor': avg_bus_factor,\n",
    "        'trend_direction': 'Federating' if slope > 0.001 else ('Concentrating' if slope < -0.001 else 'Stable')\n",
    "    })\n",
    "\n",
    "df_summary = pd.DataFrame(summary_data)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"TEMPORAL ANALYSIS SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "display(df_summary.round(4))\n",
    "\n",
    "# Save results\n",
    "output_path = Path('../data/processed/temporal_analysis.csv')\n",
    "output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "df_summary.to_csv(output_path, index=False)\n",
    "print(f\"\\nâœ… Summary saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-19",
   "metadata": {},
   "source": [
    "## Key Findings\n",
    "\n",
    "**Interpretation:**\n",
    "- **Positive slope**: Project is \"federating\" - contributions becoming more distributed\n",
    "- **Negative slope**: Project is \"concentrating\" - moving toward Stadium pattern\n",
    "- **Low bus factor + negative slope**: High sustainability risk\n",
    "- **Significant transitions**: May indicate key contributor departures or onboarding\n",
    "\n",
    "**Next Steps:**\n",
    "1. Correlate transitions with external events (releases, security issues)\n",
    "2. Compare temporal patterns across project categories\n",
    "3. Build predictive model for sustainability risk"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Categories)",
   "language": "python",
   "name": "categories-env"
  },
  "language_info": {
   "name": "python",
   "version": "3.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
